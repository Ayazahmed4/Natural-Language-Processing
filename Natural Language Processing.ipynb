{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc8efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d4c97be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41237179",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have a dream that one day down in Alabama, \n",
    "                with its vicious racists, with its governor \n",
    "                having his lips dripping with the words of \n",
    "                interposition and nullification - \n",
    "                one day right there in Alabama little black boys \n",
    "                and black girls will be able to join hands with \n",
    "                little white boys and white girls as \n",
    "                sisters and brothers.\n",
    "                I have a dream today.\n",
    "                I have a dream that one day every valley \n",
    "                shall be exalted, and every hill and \n",
    "                mountain shall be made low, \n",
    "                the rough places will be made plain, \n",
    "                and the crooked places will be made straight, \n",
    "                and the glory of the Lord shall be revealed \n",
    "                and all flesh shall see it together.\n",
    "                This is our hope. \n",
    "                This is the faith that I go back to the South with. \n",
    "                With this faith we will be able to hew out \n",
    "                of the mountain of despair a stone of hope. \n",
    "                With this faith we will be able to transform \n",
    "                the jangling discords of our nation into a \n",
    "                beautiful symphony of brotherhood. \n",
    "                With this faith we will be able to work together, \n",
    "                to pray together, to struggle together, \n",
    "                to go to jail together, to stand up for freedom together, \n",
    "                knowing that we will be free one day.\n",
    "                This will be the day, this will be the day when \n",
    "                all of God's children will be able to sing \n",
    "                with new meaning \n",
    "                \"My country 'tis of thee, sweet land of liberty, \n",
    "                of thee I sing. Land where my father's died, \n",
    "                land of the Pilgrim's pride, from every mountainside, \n",
    "                let freedom ring!\"\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa17c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a dream that one day down in Alabama, \n",
      "                with its vicious racists, with its governor \n",
      "                having his lips dripping with the words of \n",
      "                interposition and nullification - \n",
      "                one day right there in Alabama little black boys \n",
      "                and black girls will be able to join hands with \n",
      "                little white boys and white girls as \n",
      "                sisters and brothers.\n",
      "                I have a dream today.\n",
      "                I have a dream that one day every valley \n",
      "                shall be exalted, and every hill and \n",
      "                mountain shall be made low, \n",
      "                the rough places will be made plain, \n",
      "                and the crooked places will be made straight, \n",
      "                and the glory of the Lord shall be revealed \n",
      "                and all flesh shall see it together.\n",
      "                This is our hope. \n",
      "                This is the faith that I go back to the South with. \n",
      "                With this faith we will be able to hew out \n",
      "                of the mountain of despair a stone of hope. \n",
      "                With this faith we will be able to transform \n",
      "                the jangling discords of our nation into a \n",
      "                beautiful symphony of brotherhood. \n",
      "                With this faith we will be able to work together, \n",
      "                to pray together, to struggle together, \n",
      "                to go to jail together, to stand up for freedom together, \n",
      "                knowing that we will be free one day.\n",
      "                This will be the day, this will be the day when \n",
      "                all of God's children will be able to sing \n",
      "                with new meaning \n",
      "                \"My country 'tis of thee, sweet land of liberty, \n",
      "                of thee I sing. Land where my father's died, \n",
      "                land of the Pilgrim's pride, from every mountainside, \n",
      "                let freedom ring!\"\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cfee48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 10\n",
      "I have a dream that one day down in Alabama, \n",
      "                with its vicious racists, with its governor \n",
      "                having his lips dripping with the words of \n",
      "                interposition and nullification - \n",
      "                one day right there in Alabama little black boys \n",
      "                and black girls will be able to join hands with \n",
      "                little white boys and white girls as \n",
      "                sisters and brothers.\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZATION\n",
    "# Tokenizing sentences - Converting to sentences\n",
    "sentences_1 = nltk.sent_tokenize(paragraph)\n",
    "print(\"Number of sentences:\",len(sentences_1))\n",
    "print(sentences_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18b3b5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 291\n",
      "['I', 'have', 'a', 'dream', 'that', 'one', 'day', 'down', 'in', 'Alabama', ',', 'with', 'its', 'vicious', 'racists', ',', 'with', 'its', 'governor', 'having', 'his', 'lips', 'dripping', 'with', 'the', 'words', 'of', 'interposition', 'and', 'nullification', '-', 'one', 'day', 'right', 'there', 'in', 'Alabama', 'little', 'black', 'boys', 'and', 'black', 'girls', 'will', 'be', 'able', 'to', 'join', 'hands', 'with', 'little', 'white', 'boys', 'and', 'white', 'girls', 'as', 'sisters', 'and', 'brothers', '.', 'I', 'have', 'a', 'dream', 'today', '.', 'I', 'have', 'a', 'dream', 'that', 'one', 'day', 'every', 'valley', 'shall', 'be', 'exalted', ',', 'and', 'every', 'hill', 'and', 'mountain', 'shall', 'be', 'made', 'low', ',', 'the', 'rough', 'places', 'will', 'be', 'made', 'plain', ',', 'and', 'the', 'crooked', 'places', 'will', 'be', 'made', 'straight', ',', 'and', 'the', 'glory', 'of', 'the', 'Lord', 'shall', 'be', 'revealed', 'and', 'all', 'flesh', 'shall', 'see', 'it', 'together', '.', 'This', 'is', 'our', 'hope', '.', 'This', 'is', 'the', 'faith', 'that', 'I', 'go', 'back', 'to', 'the', 'South', 'with', '.', 'With', 'this', 'faith', 'we', 'will', 'be', 'able', 'to', 'hew', 'out', 'of', 'the', 'mountain', 'of', 'despair', 'a', 'stone', 'of', 'hope', '.', 'With', 'this', 'faith', 'we', 'will', 'be', 'able', 'to', 'transform', 'the', 'jangling', 'discords', 'of', 'our', 'nation', 'into', 'a', 'beautiful', 'symphony', 'of', 'brotherhood', '.', 'With', 'this', 'faith', 'we', 'will', 'be', 'able', 'to', 'work', 'together', ',', 'to', 'pray', 'together', ',', 'to', 'struggle', 'together', ',', 'to', 'go', 'to', 'jail', 'together', ',', 'to', 'stand', 'up', 'for', 'freedom', 'together', ',', 'knowing', 'that', 'we', 'will', 'be', 'free', 'one', 'day', '.', 'This', 'will', 'be', 'the', 'day', ',', 'this', 'will', 'be', 'the', 'day', 'when', 'all', 'of', 'God', \"'s\", 'children', 'will', 'be', 'able', 'to', 'sing', 'with', 'new', 'meaning', '``', 'My', 'country', \"'t\", 'is', 'of', 'thee', ',', 'sweet', 'land', 'of', 'liberty', ',', 'of', 'thee', 'I', 'sing', '.', 'Land', 'where', 'my', 'father', \"'s\", 'died', ',', 'land', 'of', 'the', 'Pilgrim', \"'s\", 'pride', ',', 'from', 'every', 'mountainside', ',', 'let', 'freedom', 'ring', '!', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing words - Converting to words\n",
    "words = nltk.word_tokenize(paragraph)\n",
    "print(\"Number of words:\",len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db65a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEMMING\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f263b696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing sentences\n",
    "sentences_2 = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebc6dfa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20963c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94eab8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming function\n",
    "sentences_stem = sentences_2.copy()\n",
    "for i in range(len(sentences_2)):\n",
    "    words = nltk.word_tokenize(sentences_2[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences_stem[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84527335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a dream that one day down in Alabama, \n",
      "                with its vicious racists, with its governor \n",
      "                having his lips dripping with the words of \n",
      "                interposition and nullification - \n",
      "                one day right there in Alabama little black boys \n",
      "                and black girls will be able to join hands with \n",
      "                little white boys and white girls as \n",
      "                sisters and brothers.\n",
      "i dream one day alabama , viciou racist , governor lip drip word interposit nullif - one day right alabama littl black boy black girl abl join hand littl white boy white girl sister brother .\n"
     ]
    }
   ],
   "source": [
    "# Sentences after stemming\n",
    "print(sentences_2[0])\n",
    "print(sentences_stem[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b0a3dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEMMATIZATION\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9815299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing sentences\n",
    "sentences_3 = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caf6afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e16fdc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer function\n",
    "sentences_lemm = sentences_3.copy()\n",
    "for i in range(len(sentences_3)):\n",
    "    words = nltk.word_tokenize(sentences_3[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences_lemm[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5462498e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a dream that one day down in Alabama, \n",
      "                with its vicious racists, with its governor \n",
      "                having his lips dripping with the words of \n",
      "                interposition and nullification - \n",
      "                one day right there in Alabama little black boys \n",
      "                and black girls will be able to join hands with \n",
      "                little white boys and white girls as \n",
      "                sisters and brothers.\n",
      "I dream one day Alabama , vicious racist , governor lip dripping word interposition nullification - one day right Alabama little black boy black girl able join hand little white boy white girl sister brother .\n"
     ]
    }
   ],
   "source": [
    "# Sentences after Lemmatization\n",
    "print(sentences_3[0])\n",
    "print(sentences_lemm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "094ee525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAG OF WORDS\n",
    "# Cleaning the text\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59c56770",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wordnet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f35270cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a dream that one day down in Alabama, \n",
      "                with its vicious racists, with its governor \n",
      "                having his lips dripping with the words of \n",
      "                interposition and nullification - \n",
      "                one day right there in Alabama little black boys \n",
      "                and black girls will be able to join hands with \n",
      "                little white boys and white girls as \n",
      "                sisters and brothers.\n"
     ]
    }
   ],
   "source": [
    "sentences_4 = nltk.sent_tokenize(paragraph)\n",
    "print(sentences_4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d328d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the bag of words corpus after cleaning\n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3928251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning function\n",
    "for i in range(len(sentences_4)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences_4[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9501c4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a dream that one day down in Alabama, \n",
      "                with its vicious racists, with its governor \n",
      "                having his lips dripping with the words of \n",
      "                interposition and nullification - \n",
      "                one day right there in Alabama little black boys \n",
      "                and black girls will be able to join hands with \n",
      "                little white boys and white girls as \n",
      "                sisters and brothers.\n",
      "dream one day alabama vicious racist governor lip dripping word interposition nullification one day right alabama little black boy black girl able join hand little white boy white girl sister brother\n"
     ]
    }
   ],
   "source": [
    "# Sentences after Cleaning\n",
    "print(sentences_4[0])\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1b12d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 0 0 2 2 1 0 0 0 0 2 0 0 0 1 1 0 0 0 0 0 0 0 2 0 0 0 1 1 0 0 0 1 0 0\n",
      "  1 0 0 0 0 1 2 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 2 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 2 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 1 3 0 1 0 0 0 0 1 0 2 1 0 0 0 1 0 0 1 1 4 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
      "  0 0 0 0 0 5 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0\n",
      "  1 0 2 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 2 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Creating the Bag of words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X_bow = cv.fit_transform(corpus).toarray()\n",
    "print(X_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dca7274e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09177377 0.30909581 0.         0.         0.30909581 0.30909581\n",
      "  0.1545479  0.         0.         0.         0.         0.20438305\n",
      "  0.         0.         0.         0.11494183 0.1545479  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.30909581 0.         0.         0.         0.1545479  0.1545479\n",
      "  0.         0.         0.         0.1545479  0.         0.\n",
      "  0.1545479  0.         0.         0.         0.         0.1545479\n",
      "  0.30909581 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.1545479  0.22988367 0.\n",
      "  0.         0.         0.         0.         0.1545479  0.\n",
      "  0.1545479  0.         0.         0.         0.         0.\n",
      "  0.1545479  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.1545479  0.30909581 0.1545479  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.59677497 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.80240865 0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.1445192  0.09556026\n",
      "  0.         0.         0.         0.10748319 0.         0.24570905\n",
      "  0.1445192  0.         0.         0.1445192  0.         0.\n",
      "  0.         0.1445192  0.         0.         0.         0.\n",
      "  0.         0.1445192  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.1445192  0.1445192  0.4335576  0.         0.12285452\n",
      "  0.         0.         0.         0.         0.10748319 0.\n",
      "  0.2890384  0.1445192  0.         0.         0.         0.1445192\n",
      "  0.         0.         0.1445192  0.1445192  0.5780768  0.\n",
      "  0.         0.         0.         0.         0.1445192  0.\n",
      "  0.         0.         0.         0.         0.         0.12285452\n",
      "  0.         0.1445192  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.56255473 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.3719774  0.         0.         0.         0.\n",
      "  0.         0.         0.47822292 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.56255473 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.25953179 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.43705403 0.         0.         0.         0.         0.\n",
      "  0.         0.28899272 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.43705403 0.         0.37153585 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.37153585\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.43705403 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.21276049 0.         0.         0.35829071 0.         0.\n",
      "  0.         0.35829071 0.         0.         0.         0.\n",
      "  0.         0.         0.35829071 0.         0.         0.\n",
      "  0.         0.23691214 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.35829071\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.35829071 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.35829071 0.         0.         0.         0.\n",
      "  0.35829071 0.         0.         0.         0.         0.        ]\n",
      " [0.11164118 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.1243142\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.1243142  0.         0.         0.18800481 0.15982127\n",
      "  0.         0.         0.15982127 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.18800481 0.\n",
      "  0.         0.18800481 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.13982472 0.\n",
      "  0.         0.         0.18800481 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.18800481 0.         0.         0.18800481\n",
      "  0.         0.         0.         0.         0.         0.79910633\n",
      "  0.         0.         0.         0.         0.         0.18800481]\n",
      " [0.1368666  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.23048467 0.23048467 0.         0.30480621\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.23048467 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.19593303 0.         0.23048467 0.\n",
      "  0.         0.         0.         0.         0.23048467 0.\n",
      "  0.         0.         0.23048467 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.46096935\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.23048467 0.         0.46096935 0.23048467 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.29701021 0.         0.         0.         0.25248582\n",
      "  0.         0.         0.29701021 0.         0.         0.25248582\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.50497163 0.29701021 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.29701021 0.         0.         0.         0.         0.29701021\n",
      "  0.         0.         0.         0.29701021 0.         0.\n",
      "  0.         0.29701021 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# TERM FREQUENCY AND INVERSE DOCUMENT FREQUENCY - TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer()\n",
    "X_tv = tv.fit_transform(corpus).toarray()\n",
    "print(X_tv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
